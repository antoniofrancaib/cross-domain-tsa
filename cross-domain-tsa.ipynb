{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e15a1c1c-2b2c-402c-8db7-b66dadef6f1b",
   "metadata": {},
   "source": [
    "# Cross-Domain Sentiment Classification with Domain-Adaptive Neural Networks\n",
    "\n",
    "## Project Overview\n",
    "\n",
    "Sentiment analysis, the computational study of opinions expressed in text, has vast applications in understanding customer feedback, social media monitoring, and opinion mining. However, the performance of sentiment analysis models can significantly drop when applied to a new domain due to the domain discrepancy. This project aims to tackle this challenge using domain adaptation techniques in neural networks.\n",
    "\n",
    "## Objective\n",
    "\n",
    "The primary objective of this project is to develop a neural network capable of adapting the knowledge from one domain and effectively applying it to a different domain. Specifically, we will train our model on the IMDB movie review dataset and adapt it to analyze sentiments in the YELP restaurant review dataset.\n",
    "\n",
    "## Approach\n",
    "\n",
    "Train model primarily on the source domain data (IMDb) and use a smaller subset of the target domain data (Yelp) for domain adaptation techniques. Here's a common strategy to set up the data:\n",
    "\n",
    "Train primarily on the source domain: Use all (or a large subset) of the source domain data for training the feature extractor and sentiment classifier.\n",
    "\n",
    "Use a small subset of the target domain for adaptation: Include a smaller, balanced sample of the target domain data during training to help the model learn features that are useful for both domains. This is where techniques like the gradient reversal layer come in, to encourage the model to learn domain-agnostic features.\n",
    "\n",
    "Further fine-tune on the target domain if necessary: After the model has been trained with the combined source and a small portion of the target domain data, you might optionally fine-tune the model further on a larger portion of the target domain data to improve performance on that specific domain.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d4ca0c0-5b71-457b-87a1-7fc71606249c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Initialize Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "a93dedcc-ca5a-412d-9905-c1d24ceccfbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters \n",
    "yelp_sample_size = 10000  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa24857-51b0-4fa2-b287-0068b5565822",
   "metadata": {},
   "source": [
    "## IMDB Data ∼ Domain Source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52b8afef-1e2e-49d3-bc19-089d4290f25e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6b3772a-2d8d-4876-b681-838de0e6a709",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_df = pd.read_csv('IMDB Dataset.csv')\n",
    "imdb_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74af136e-59e8-44c7-b87b-f1134df475c3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentiment\n",
      "positive    25000\n",
      "negative    25000\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "sentiment_counts_imdb = imdb_df['sentiment'].value_counts()\n",
    "print(sentiment_counts_imdb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d182c747-165c-49ae-ab57-b7e29ede4a2d",
   "metadata": {},
   "source": [
    "## Yelp Data ∼ Target Source\n",
    "\n",
    "We consider ratings of 4 and 5 stars as positive and ratings of 1 and 2 stars as negative. We discard 3-star reviews as they are neutral. Alternatively, we might include them in further explorations in one of the categories based on the needs of our analysis.\n",
    "\n",
    "https://www.kaggle.com/datasets/yelp-dataset/yelp-dataset/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "786475fe-b2a9-4024-8444-510eb77e895a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "data_file = open(\"yelp_academic_dataset_review.json\")\n",
    "review_df = []\n",
    "for line in data_file:\n",
    "    review_df.append(json.loads(line))\n",
    "yelp_df = pd.DataFrame(review_df)\n",
    "data_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b85d84ec-c813-4917-9375-2c51971b1988",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I've taken a lot of spin classes over the year...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Wow!  Yummy, different,  delicious.   Our favo...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Cute interior and owner (?) gave us tour of up...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>I am a long term frequent customer of this est...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Loved this tour! I grabbed a groupon and the p...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "1  I've taken a lot of spin classes over the year...  positive\n",
       "3  Wow!  Yummy, different,  delicious.   Our favo...  positive\n",
       "4  Cute interior and owner (?) gave us tour of up...  positive\n",
       "5  I am a long term frequent customer of this est...  negative\n",
       "6  Loved this tour! I grabbed a groupon and the p...  positive"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filter out rows where 'stars' is 3 \n",
    "yelp_df = yelp_df[yelp_df['stars'] != 3.0].copy()\n",
    "\n",
    "yelp_df['sentiment'] = yelp_df['stars'].apply(lambda x: 'positive' if x >= 4 else 'negative')\n",
    "yelp_df = yelp_df.rename(columns={'text': 'review'})\n",
    "\n",
    "yelp_df = yelp_df[['review', 'sentiment']]\n",
    "\n",
    "yelp_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "554d5984-ef71-40f6-bafe-07909e2ab103",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentiment\n",
      "positive    4684545\n",
      "negative    1613801\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# ideally, we want symmetry here, also, for simplicity, we will use a subset of this data\n",
    "sentiment_counts = yelp_df['sentiment'].value_counts()\n",
    "print(sentiment_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "3a2d9af6-8fbe-4dc1-9671-44469b08870a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampling an equal number of positive and negative reviews\n",
    "yelp_positive_sample = yelp_df[yelp_df['sentiment'] == 'positive'].sample(n=yelp_sample_size // 2, random_state=42)\n",
    "yelp_negative_sample = yelp_df[yelp_df['sentiment'] == 'negative'].sample(n=yelp_sample_size // 2, random_state=42)\n",
    "\n",
    "yelp_balanced_sample = pd.concat([yelp_positive_sample, yelp_negative_sample]).sample(frac=1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "075059e7-8154-46b2-8ac6-f803fbe7bd28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentiment\n",
      "negative    5000\n",
      "positive    5000\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "sentiment_counts = yelp_balanced_sample['sentiment'].value_counts()\n",
    "print(sentiment_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc33fc2-bc0f-4604-9191-95e21b070a1f",
   "metadata": {},
   "source": [
    "## Combine Data to create the final Training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "f82e4c18-b6c6-4082-8fdb-2df06e6b8900",
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_df['domain'] = 0\n",
    "yelp_balanced_sample['domain'] = 1\n",
    "\n",
    "combined_df = pd.concat([imdb_df, yelp_balanced_sample])\n",
    "\n",
    "combined_df = combined_df.sample(frac=1, random_state=42).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "843909a3-d7bd-4dbb-afd7-cbd2f49a32e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>domain</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>paula may bitch never butch br br hilari line ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mani peopl say show kid hm kid approxim 7 9 ye...</td>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>well written tale make batman sitcom actual re...</td>\n",
       "      <td>positive</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>think movi absolut beauti refer breathtak scen...</td>\n",
       "      <td>positive</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>film outstand despit nc 17 rate disturb scene ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59995</th>\n",
       "      <td>The girl at the front desk was on the phone, I...</td>\n",
       "      <td>negative</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59996</th>\n",
       "      <td>avoid one terribl movi excit pointless murder ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59997</th>\n",
       "      <td>product quit surpri absolut love obscur earli ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59998</th>\n",
       "      <td>decent movi although littl bit short time pack...</td>\n",
       "      <td>positive</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59999</th>\n",
       "      <td>It's a very spacious eatery with ordering stat...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>60000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review sentiment  domain\n",
       "0      paula may bitch never butch br br hilari line ...  negative       0\n",
       "1      mani peopl say show kid hm kid approxim 7 9 ye...  negative       0\n",
       "2      well written tale make batman sitcom actual re...  positive       0\n",
       "3      think movi absolut beauti refer breathtak scen...  positive       0\n",
       "4      film outstand despit nc 17 rate disturb scene ...  positive       0\n",
       "...                                                  ...       ...     ...\n",
       "59995  The girl at the front desk was on the phone, I...  negative       1\n",
       "59996  avoid one terribl movi excit pointless murder ...  negative       0\n",
       "59997  product quit surpri absolut love obscur earli ...  positive       0\n",
       "59998  decent movi although littl bit short time pack...  positive       0\n",
       "59999  It's a very spacious eatery with ordering stat...  positive       1\n",
       "\n",
       "[60000 rows x 3 columns]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "338fedab-52fe-4317-aae0-096716d364bf",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 1. Initial Model Training with Source Domain (IMDB)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29272c34-e42f-4aa1-a996-eb213dcd8c1c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Data Preprocessing for IMDB: \n",
    "<span style=\"color:red\">Status: </span> <span style=\"color:blue\">ALMOST FINISHED: </span>This includes text cleaning, tokenization, and padding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ca1110-6a63-4658-8871-c2d1d15c0e0c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Clean & Normalization \n",
    "Let's first clean the texts like removing stopwords, special characters, stemming, and lemmatization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4ee4d44-d986-4a2c-aebc-2c5011829908",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk import word_tokenize, pos_tag\n",
    "\n",
    "import string\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000b7a18-d8f5-47cc-b8fd-6eb33d6d0718",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e65eac2a-8df6-49d1-9ebf-fadfddd8d28c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    # remove stop words like \"the\", \"is\", \"in\", \"on\", \"and\", \"but\", etc. \n",
    "    # the focus is on the more meaningful words that give insight into the content.\n",
    "    stop_words = stopwords.words('english')\n",
    "    words = text.split()\n",
    "    filtered_sentence = ''\n",
    "    for word in words:\n",
    "        if word not in stop_words:\n",
    "            filtered_sentence = filtered_sentence + word + ' '\n",
    "    return filtered_sentence\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    table = str.maketrans('','',string.punctuation)\n",
    "    words = text.split()\n",
    "    filtered_sentence = ''\n",
    "    for word in words:\n",
    "        word = word.translate(table)\n",
    "        filtered_sentence = filtered_sentence + word + ' '\n",
    "    return filtered_sentence\n",
    "\n",
    "def normalize_text(text):\n",
    "    text = text.lower()\n",
    "    # get rid of urls\n",
    "    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
    "    # get rid of non words and extra spaces\n",
    "    text = re.sub('\\\\W', ' ', text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "    text = re.sub(' +', ' ', text)\n",
    "    text = re.sub('^ ', '', text)\n",
    "    text = re.sub(' $', '', text)\n",
    "    return text\n",
    "\n",
    "def stemming(text):\n",
    "    ps = PorterStemmer()\n",
    "    words = text.split()\n",
    "    filtered_sentence = ''\n",
    "    for word in words:\n",
    "        word = ps.stem(word)\n",
    "        filtered_sentence = filtered_sentence + word + ' '\n",
    "    return filtered_sentence\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = text.replace(',',' , ')\n",
    "    text = text.replace('.',' . ')\n",
    "    text = text.replace('/',' / ')\n",
    "    text = text.replace('@',' @ ')\n",
    "    text = text.replace('#',' # ')\n",
    "    text = text.replace('?',' ? ')\n",
    "    text = normalize_text(text)\n",
    "    text = remove_punctuation(text)\n",
    "    text = remove_stopwords(text)\n",
    "    text = stemming(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "56568291-dba4-4d9c-a7f2-fb1577b6872b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "combined_df['review'] = combined_df['review'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "338c9ab8-11ce-426a-b072-0e3ca0636bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_raw = imdb_df['review']\n",
    "y_train_raw = imdb_df['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "f6f55fd5-4522-45f1-87a8-208b8125be12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "inputs = tokenizer(list(combined_df['review']), padding=True, truncation=True, return_tensors=\"pt\", max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "454debff-5202-4bb8-a291-af99c586b1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_mapping = {'positive': 1, 'negative': 0}\n",
    "\n",
    "combined_df['sentiment'] = combined_df['sentiment'].map(sentiment_mapping)\n",
    "\n",
    "sentiments = torch.tensor(combined_df['sentiment'].values, dtype=torch.long)\n",
    "domains = torch.tensor(combined_df['domain'].values, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "b8756cfb-8872-458a-9388-381f3513df9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataset = TensorDataset(inputs['input_ids'], inputs['attention_mask'], sentiments, domains)\n",
    "\n",
    "train_size = int(0.8 * len(full_dataset))\n",
    "val_size = len(full_dataset) - train_size\n",
    "\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(full_dataset, [train_size, val_size])\n",
    "\n",
    "batch_size = 16\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089494ce-5930-416d-a9d6-fc8e94cbc51d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Build Model: \n",
    "<span style=\"color:red\">Status: </span> <span style=\"color:blue\">TO DO: </span> Design a neural network architecture suitable for sentiment analysis, e.g., LSTM, GRU, or even a transformer-based model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "95623b79-7401-40aa-b836-a21fdb46a160",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel, BertTokenizer\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "# Define the model architecture\n",
    "class SentimentDomainModel(nn.Module):\n",
    "    def __init__(self, bert_model_name, hidden_size, sentiment_classes, domain_classes):\n",
    "        super(SentimentDomainModel, self).__init__()\n",
    "        \n",
    "        # Feature Extractor\n",
    "        self.bert = BertModel.from_pretrained(bert_model_name)\n",
    "        \n",
    "        # Sentiment Classifier\n",
    "        self.sentiment_classifier = nn.Sequential(\n",
    "            nn.Dropout(p=0.1),\n",
    "            nn.Linear(self.bert.config.hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, sentiment_classes)\n",
    "        )\n",
    "        \n",
    "        # Domain Classifier\n",
    "        self.domain_classifier = nn.Sequential(\n",
    "            nn.Dropout(p=0.1),\n",
    "            nn.Linear(self.bert.config.hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, domain_classes)\n",
    "        )\n",
    "\n",
    "        self.gradient_reversal_alpha = 1.0  # Define the negative constant for the gradient reversal layer\n",
    "\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        # BERT Feature Extraction\n",
    "        outputs = self.bert(input_ids=input_ids,\n",
    "                            attention_mask=attention_mask,\n",
    "                            token_type_ids=token_type_ids)\n",
    "        \n",
    "        pooled_output = outputs.pooler_output\n",
    "\n",
    "        # Apply the gradient reversal layer with the chosen alpha\n",
    "        reversed_features = gradient_reversal(pooled_output, self.gradient_reversal_alpha)\n",
    "        \n",
    "        # Sentiment classification\n",
    "        sentiment_output = self.sentiment_classifier(pooled_output)\n",
    "        \n",
    "        # Domain classification\n",
    "        domain_output = self.domain_classifier(reversed_features)\n",
    "        \n",
    "        return sentiment_output, domain_output\n",
    "\n",
    "# Initialize the model\n",
    "bert_model_name = 'bert-base-uncased'  # You can choose other BERT models as needed\n",
    "hidden_size = 128  # Size of the hidden layer for both classifiers\n",
    "sentiment_classes = 2  # Assuming binary classification for sentiment\n",
    "domain_classes = 2  # Assuming binary classification for domain\n",
    "\n",
    "model = SentimentDomainModel(bert_model_name, hidden_size, sentiment_classes, domain_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80565586-2c04-4de3-835e-24246459d748",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Assuming you have already defined the SentimentDomainModel as above\n",
    "\n",
    "# Criterion for sentiment and domain classification\n",
    "sentiment_criterion = nn.CrossEntropyLoss()\n",
    "domain_criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=2e-5)\n",
    "\n",
    "# Number of epochs\n",
    "num_epochs = 3\n",
    "\n",
    "# Training and evaluation function\n",
    "def train_model(model, train_loader, val_loader, num_epochs, sentiment_criterion, domain_criterion, optimizer):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_sentiment_loss = 0\n",
    "        total_domain_loss = 0\n",
    "\n",
    "        for batch in train_loader:\n",
    "            # Unpack the batch\n",
    "            input_ids, attention_mask, sentiments, domains = batch\n",
    "\n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            sentiment_outputs, domain_outputs = model(input_ids, attention_mask, None)\n",
    "\n",
    "            # Compute loss\n",
    "            sentiment_loss = sentiment_criterion(sentiment_outputs, sentiments)\n",
    "            domain_loss = domain_criterion(domain_outputs, domains)\n",
    "\n",
    "            # Combine losses and backward pass\n",
    "            total_loss = sentiment_loss + domain_loss\n",
    "            total_loss.backward()\n",
    "\n",
    "            # Update parameters\n",
    "            optimizer.step()\n",
    "\n",
    "            # Statistics\n",
    "            total_sentiment_loss += sentiment_loss.item()\n",
    "            total_domain_loss += domain_loss.item()\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        total_val_loss = 0\n",
    "        correct = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                input_ids, attention_mask, sentiments, domains = batch\n",
    "                \n",
    "                # Forward pass\n",
    "                sentiment_outputs, domain_outputs = model(input_ids, attention_mask, None)\n",
    "                \n",
    "                # Compute loss\n",
    "                sentiment_loss = sentiment_criterion(sentiment_outputs, sentiments)\n",
    "                domain_loss = domain_criterion(domain_outputs, domains)\n",
    "                val_loss = sentiment_loss + domain_loss\n",
    "\n",
    "                total_val_loss += val_loss.item()\n",
    "\n",
    "                # Sentiment accuracy\n",
    "                _, predicted = torch.max(sentiment_outputs.data, 1)\n",
    "                correct += (predicted == sentiments).sum().item()\n",
    "\n",
    "        val_accuracy = correct / len(val_loader.dataset)\n",
    "\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, '\n",
    "              f'Train Loss: Sentiment {total_sentiment_loss:.4f} Domain {total_domain_loss:.4f}, '\n",
    "              f'Val Loss: {total_val_loss:.4f}, Accuracy: {val_accuracy:.4f}')\n",
    "\n",
    "    # Save the model checkpoint\n",
    "    torch.save(model.state_dict(), 'sentiment_domain_model.pth')\n",
    "\n",
    "# Call the training function\n",
    "train_model(model, train_loader, val_loader, num_epochs, sentiment_criterion, domain_criterion, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52920d6d-71df-4b36-9d54-ca35f473116c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Text Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c9073e-3845-4e89-89f6-99fac84ffad8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install torch==2.0.0 torchtext==0.15.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f44c913-d6d2-480c-beca-ad2d06e7f08a",
   "metadata": {},
   "source": [
    "Pre-trained Word2Vec models have been trained on large datasets and can capture the semantic meaning of words quite well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3bdf9b3a-e1ab-4f12-bef3-c66cb9698030",
   "metadata": {},
   "outputs": [],
   "source": [
    "# word embedding\n",
    "import gensim.downloader as api\n",
    "\n",
    "word2vec_model = api.load(\"word2vec-google-news-300\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07281edc-91bd-4401-94e5-dceae7cd70c0",
   "metadata": {},
   "source": [
    "Regarding the aggregation method: using the mean is a common and straightforward approach, but it's true that other methods could potentially capture more information. Some possible alternatives include:\n",
    "\n",
    "Summation: Instead of averaging the word vectors, sum them. This may give more weight to longer texts.\n",
    "TF-IDF Weighting: Weight the word vectors by their term frequency-inverse document frequency scores before averaging or summing, to give more importance to distinctive words.\n",
    "Max Pooling: Take the maximum value across each dimension of the word vectors to form the text vector.\n",
    "Concatenation: Concatenate the average, max, and min vectors.\n",
    "Hierarchical Pooling: Use a more complex pooling strategy that combines vectors in a hierarchical structure to retain more information.\n",
    "Paragraph Vectors: Train a model like Doc2Vec that learns fixed-length feature representations for variable-length pieces of texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e3b5f5b1-0470-42be-bf52-b30d0c9c3786",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_word2vec(text, model):\n",
    "    words = word_tokenize(text)\n",
    "    vector_list = [model[word] for word in words if word in model]\n",
    "\n",
    "    if not vector_list: \n",
    "        return np.zeros(model.vector_size)\n",
    "    \n",
    "    # Aggregation method: mean vector for the text\n",
    "    return np.mean(vector_list, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0b4c479c-dfb1-4688-b71d-7197f8518100",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_word2vec = np.array([text_to_word2vec(text, word2vec_model) for text in X_train_raw])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553d6e21-f0cb-4c8e-9ffd-f195c9828c40",
   "metadata": {},
   "source": [
    "> Labels Encoding and Dataset Splitting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c382f894-b942-49ea-abb8-9614b9b19d9a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "75e89d97-757d-47f5-8dfd-55c7d246ad1c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "one = OneHotEncoder()\n",
    "y_train_onehot = one.fit_transform(np.asarray(y_train_raw).reshape(-1, 1)).toarray()\n",
    "y_train_tensor = torch.tensor(y_train_onehot, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "37930a31-9dbe-4db9-8c1d-33e09efb0f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train_word2vec, \n",
    "    y_train_tensor.numpy(), \n",
    "    test_size=0.2,  \n",
    "    random_state=42  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "72216d52-ae92-40f0-b386-9d07a537c061",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "8781d44e-a5fa-474e-a949-c90b386a28de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "8f67a044-4b53-4b39-a847-a4b87e1cc3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Function\n",
    "\n",
    "class GradientReversalFunction(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, alpha):\n",
    "        ctx.alpha = alpha\n",
    "        return x.view_as(x)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        output = grad_output.neg() * ctx.alpha\n",
    "        # Return gradient for input, None for alpha\n",
    "        return output, None\n",
    "\n",
    "# Alias to easily use the function without passing an alpha each time\n",
    "def gradient_reversal(x, alpha=1.0):\n",
    "    return GradientReversalFunction.apply(x, alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "ad92b6fd-3d38-4612-9cd5-c7dfa5ecde78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "355e8f85-0900-4234-92cc-de2a167caa21",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[112], line 21\u001b[0m\n\u001b[1;32m     18\u001b[0m total_sentiment_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     19\u001b[0m total_domain_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 21\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m inputs, sentiments, domains \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;66;03m# Zero the parameter gradients\u001b[39;00m\n\u001b[1;32m     23\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 3)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from transformers import BertModel\n",
    "\n",
    "# Criterion for sentiment and domain classification\n",
    "sentiment_criterion = nn.CrossEntropyLoss()\n",
    "domain_criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Optimizers\n",
    "optimizer = Adam(model.parameters(), lr=1e-5)\n",
    "\n",
    "# Number of epochs\n",
    "num_epochs = 3\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_sentiment_loss = 0\n",
    "    total_domain_loss = 0\n",
    "\n",
    "    for inputs, sentiments, domains in train_loader:\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        sentiment_outputs, domain_outputs = model(inputs['input_ids'], inputs['attention_mask'])\n",
    "\n",
    "        # Compute loss\n",
    "        sentiment_loss = sentiment_criterion(sentiment_outputs, sentiments)\n",
    "        domain_loss = domain_criterion(domain_outputs, domains)\n",
    "\n",
    "        # Combine losses\n",
    "        total_loss = sentiment_loss + domain_loss\n",
    "        total_loss.backward()\n",
    "\n",
    "        # Update parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # Statistics\n",
    "        total_sentiment_loss += sentiment_loss.item()\n",
    "        total_domain_loss += domain_loss.item()\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, sentiments, domains in val_loader:\n",
    "            sentiment_outputs, domain_outputs = model(inputs['input_ids'], inputs['attention_mask'])\n",
    "            sentiment_loss = sentiment_criterion(sentiment_outputs, sentiments)\n",
    "            domain_loss = domain_criterion(domain_outputs, domains)\n",
    "            val_loss = sentiment_loss + domain_loss\n",
    "\n",
    "            total_val_loss += val_loss.item()\n",
    "\n",
    "            # Sentiment accuracy\n",
    "            _, predicted = torch.max(sentiment_outputs.data, 1)\n",
    "            correct += (predicted == sentiments).sum().item()\n",
    "\n",
    "    val_accuracy = correct / len(val_dataset)\n",
    "\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, '\n",
    "          f'Train Loss: Sentiment {total_sentiment_loss:.4f} Domain {total_domain_loss:.4f}, '\n",
    "          f'Val Loss: {total_val_loss:.4f}, Accuracy: {val_accuracy:.4f}')\n",
    "\n",
    "# Save the model checkpoint\n",
    "torch.save(model.state_dict(), 'sentiment_domain_model.pth')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408c4864-e042-447b-afa8-f80ee211d511",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## (LINEAR) Train Model on IMDB: \n",
    "<span style=\"color:red\">Status: </span> <span style=\"color:blue\">TO DO: </span> Using the IMDB dataset, train your model until it achieves satisfactory performance. This trained model captures the characteristics of the source domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "cab80fa2-c9df-49f5-b967-d471787c9fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, criterion, data_loader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct_predictions = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in data_loader:\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            # Adjust labels to match the output of CrossEntropyLoss\n",
    "            labels = torch.max(labels, 1)[1]\n",
    "            \n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            correct_predictions += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = (correct_predictions / len(data_loader.dataset)) * 100\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    \n",
    "    return avg_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "cd573d96-e8dc-4803-9643-f6857815eea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, train_loader, val_loader, num_epochs=100, patience=5):\n",
    "    best_val_loss = float('inf')\n",
    "    no_improvement_epochs = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        \n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            # Adjust labels to match the output of CrossEntropyLoss\n",
    "            labels = torch.max(labels, 1)[1]\n",
    "            \n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        val_loss, accuracy = evaluate_model(model, criterion, val_loader)\n",
    "        print(f\"Epoch {epoch+1}, Validation Loss: {val_loss}, Validation Accuracy: {accuracy}%\")\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            no_improvement_epochs = 0\n",
    "            # Save the best model\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "        else:\n",
    "            no_improvement_epochs += 1\n",
    "            \n",
    "        if no_improvement_epochs >= patience:\n",
    "            print(f\"Early stopping triggered after {epoch+1} epochs.\")\n",
    "            break\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "8e001a70-b5c8-463a-b074-096ab98aa3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "input_dim = 300   # Dimension of Word2Vec features\n",
    "hidden_dim = 128  # Size of the first hidden layer\n",
    "output_dim = 2    # Number of classes, assuming binary classification\n",
    "\n",
    "model = SentimentClassifier(input_dim, hidden_dim, output_dim)\n",
    "criterion = nn.CrossEntropyLoss() \n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "f446bfc9-e266-4408-aecb-b236b77f5969",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Validation Loss: 0.4208904819965363, Validation Accuracy: 80.97%\n",
      "Epoch 2, Validation Loss: 0.4089745656967163, Validation Accuracy: 81.65%\n",
      "Epoch 3, Validation Loss: 0.40679861245155335, Validation Accuracy: 81.49%\n",
      "Epoch 4, Validation Loss: 0.4046786313056946, Validation Accuracy: 81.57%\n",
      "Epoch 5, Validation Loss: 0.40317483930587766, Validation Accuracy: 81.96%\n",
      "Epoch 6, Validation Loss: 0.42194099118709566, Validation Accuracy: 80.72%\n",
      "Epoch 7, Validation Loss: 0.3963768006324768, Validation Accuracy: 82.19999999999999%\n",
      "Epoch 8, Validation Loss: 0.4460666743993759, Validation Accuracy: 79.82000000000001%\n",
      "Epoch 9, Validation Loss: 0.39371530619859696, Validation Accuracy: 82.17%\n",
      "Epoch 10, Validation Loss: 0.3936872004389763, Validation Accuracy: 82.3%\n",
      "Epoch 11, Validation Loss: 0.3916040333867073, Validation Accuracy: 82.49%\n",
      "Epoch 12, Validation Loss: 0.40407978297472, Validation Accuracy: 81.97%\n",
      "Epoch 13, Validation Loss: 0.40167471257448195, Validation Accuracy: 82.11%\n",
      "Epoch 14, Validation Loss: 0.40498597402572634, Validation Accuracy: 81.86%\n",
      "Epoch 15, Validation Loss: 0.4073795211315155, Validation Accuracy: 82.13000000000001%\n",
      "Epoch 16, Validation Loss: 0.3963420556783676, Validation Accuracy: 81.95%\n",
      "Early stopping triggered after 16 epochs.\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 100\n",
    "patience = 5  \n",
    "\n",
    "trained_model = train_model(model, criterion, optimizer, train_loader, val_loader, num_epochs=100, patience=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac57fa93-360b-4b03-89a4-0bf99d3995cd",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 2. Domain Adaptation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a00895c-34dc-4e32-92d9-4bb51c26d6a3",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Adversarial Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0bfd17",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "44d2ddc7-4696-4fd5-9edd-f42fc396a24f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "56952d6e-a457-4c74-995d-fd4a174986e5",
   "metadata": {},
   "source": [
    "# 3. Fine-tuning on Target Domain (Optional but beneficial)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722363cb-95fa-4a11-aaf5-9c93a428ae9d",
   "metadata": {},
   "source": [
    "# 4. Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d9dcb6-84e4-484d-be0b-1b4c143ba2a4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# some code I disregarded "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c075c29b-bfa2-403c-a28c-66eb2c2b5b15",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c31506f6-5a0b-4345-b4e1-f72906ae0d98",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer('basic_english')\n",
    "\n",
    "def yield_tokens(data_iter):\n",
    "    for text in data_iter:\n",
    "        yield tokenizer(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d86a73eb-dc3a-4993-b642-630833fccbfa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vocab_size = 10000\n",
    "max_length = 50\n",
    "padding_value = 0  # Index for <pad> token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "aa1bffa1-7449-41e5-820b-584397968e8e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vocab = build_vocab_from_iterator(yield_tokens(X_train), specials=['<unk>', '<pad>', '<OOV>'], max_tokens=vocab_size)\n",
    "vocab.set_default_index(vocab['<OOV>'])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "177bc9eb-8b19-4efb-9012-c54f774c2b17",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train_tokenized = [torch.tensor(vocab(tokenizer(sentence))) for sentence in X_train]\n",
    "# X_test_tokenized = [torch.tensor(vocab(tokenizer(sentence))) for sentence in X_test]\n",
    "\n",
    "X_train_padded = pad_sequence(X_train_tokenized, batch_first=True, padding_value=padding_value).to(torch.int64)\n",
    "# X_test_padded = pad_sequence(X_test_tokenized, batch_first=True, padding_value=padding_value).to(torch.int64)\n",
    "\n",
    "X_train_padded = X_train_padded[:, :max_length]\n",
    "# X_test_padded = X_test_padded[:, :max_length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a801ff-8c2a-4485-b439-301b68740d6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a34655db-af99-4234-89c5-39771bf5e4c0",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Tensorflow approach\n",
    "NO NEED TO RUN THIS PART!\n",
    "(notebook I found forinspiration: https://www.kaggle.com/code/antoniofranca/sentiment-analysis-on-imdb-movie-reviews/edit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5132aa2b-3837-4250-aea4-f88f9e272e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# important libraries for deep learning\n",
    "import tensorflow as tf \n",
    "from tensorflow import keras\n",
    "# for tokenizing texts\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "# for text padding and truncating\n",
    "from tensorflow.keras.utils import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f645f3-d433-4112-942c-a5ce5f403d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# important properties\n",
    "vocab_size = 10000\n",
    "max_length = 50\n",
    "\n",
    "trunc_type = 'post'\n",
    "padding_type = 'post'\n",
    "oov_tok = '<OOV>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b4b21f-9443-4971-8a61-2e5423f8c3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define tokenizer and fit on texts\n",
    "tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)\n",
    "tokenizer.fit_on_texts(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f5a5fe-18a4-4045-8884-8938bf9d8bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To Save conf execute this cell\n",
    "#Save Tokenizer Configuration\n",
    "import json \n",
    "import os \n",
    "\n",
    "tok_conf = tokenizer.to_json()\n",
    "\n",
    "with open('tok_conf.json', 'w') as outfile:\n",
    "    outfile.write(json.dumps(tok_conf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbfbfec9-eedf-4810-b8fc-88c5a000cfa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's Tokenize and pad texts\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "X_train = pad_sequences(X_train, maxlen=max_length,\n",
    "                         padding=padding_type,\n",
    "                         truncating=trunc_type)\n",
    "X_test = pad_sequences(X_test, maxlen=max_length,\n",
    "                         padding=padding_type,\n",
    "                         truncating=trunc_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73b5cad-abfd-4797-ba21-721e68357296",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
